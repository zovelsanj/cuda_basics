{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CUDA\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "This lab serves as an introduction to NVIDIA's accelerated computing platform, CUDA. The CUDA platform provides a general application interface for parallel computing on [CUDA-enabled GPUs](https://developer.nvidia.com/cuda-gpus). CUDA can be utilized from many programming languages, including C/C++, Fortran, and Python.\n",
    "\n",
    "In this lab you will learn:\n",
    "\n",
    "- The overall CUDA architecture, and the concept of accelerated computing\n",
    "- CUDA C++, an extension to standard C++ that enables GPU accelerated computing\n",
    "- How to accelerate common operations such as for loops on GPUs with CUDA\n",
    "- How to allocate and work with memory on GPUs\n",
    "\n",
    "After completing this lab, you will be able to take a standard serial C++ code, convert it into a parallel CUDA program, and possibly see a very significant speedup by running the program on an NVIDIA GPU. The topics we cover in this course are also applicable to other CUDA-enabled languages.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "No GPU knowledge is assumed for this lab. Basic knowledge of C/C++ is assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Computing in C++\n",
    "\n",
    "GPU computing is about massive parallelism. We'll need an appropriate example to get us started. Let's go with vector addition. We'll learn some semantic details about how to write CUDA on a toy \"hello world\" application and then implement a vector addition example.\n",
    "\n",
    "![](images/vector_addition.png)\n",
    "\n",
    "### GPU Kernels: Device Code\n",
    "\n",
    "<br />\n",
    "<span style=\"font-family:courier;font-size:2em\">\n",
    "<span style=\"color:orange\">__global__</span> <span style=\"color:green\">void</span> mykernel() { <br />}\n",
    "</span>\n",
    "\n",
    "The CUDA C++ keyword <span style=\"font-family:courier;color:orange\">\\_\\_global\\_\\_</span> indicates a function that\n",
    "\n",
    "- Runs on the device\n",
    "- Is called from host code (can also be called from other device code)\n",
    "\n",
    "The execution of the function on the device happens as a \"kernel,\" and the function itself is often commonly referred to as a \"kernel.\"\n",
    "\n",
    "`nvcc`, the NVIDIA CUDA C++ compiler driver, separates source code into host and device components\n",
    "- Device functions (e.g. `mykernel()`) processed by NVIDIA compiler\n",
    "- Host functions (e.g. `main()`) processed by standard host compiler (e.g. gcc, icc, msvc)\n",
    "\n",
    "<br />\n",
    "<span style=\"font-family:courier;font-size:2em\">\n",
    "mykernel<span style=\"color:orange\"><<<</span>1,1<span style=\"color:orange\">>>></span>();\n",
    "</span>\n",
    "\n",
    "Triple angle brackets mark a call to *device* code.\n",
    "- Also called a “kernel launch”\n",
    "- We’ll return to the parameters (1,1) in a moment\n",
    "  - For now, we'll say that these parameters inside the triple angle brackets are the CUDA kernel **execution configuration**\n",
    "\n",
    "That’s all that is required to execute a function on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's start testing out our knowledge. Edit the file [exercises/hello.cu](exercises/hello.cu) (by clicking the link to open it in a new tab, making edits directly in the source, and then using `File > Save` to save your changes) so that when it runs, it prints out the following:\n",
    "\n",
    "```\n",
    "Hello world\n",
    "```\n",
    "\n",
    "(Look for `FIXME` in the code.) \n",
    "\n",
    "Note the use of `cudaDeviceSynchronize()` after the kernel launch. In CUDA, kernel launches are asynchronous with respect to the host thread. The host thread will launch a kernel but not wait for it to finish, before proceeding with the next line of host code. Therefore, to prevent application termination before the kernel gets to print out its message, we must use this synchronization function.\n",
    "\n",
    "Once you've updated the code, compile and run it and make sure your output matches what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o hello exercises/hello.cu; ./hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, look at [solutions/hello.cu](solutions/hello.cu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Code in Parallel\n",
    "\n",
    "GPU computing is about massive parallelism. So how do we run code in parallel on the device?\n",
    "\n",
    "Instead of executing `hello()` once, we can execute it `N` times in parallel by taking\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">hello<<<<span style=\"color:orange\">1</span>,1>>>();</span>\n",
    "\n",
    "and turning it into\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">hello<<<<span style=\"color:orange\">N</span>,1>>>();</span>\n",
    "\n",
    "The parameter <span style=\"font-family:courier;color:orange\">N</span> here means we want to execute the body of the kernel  <span style=\"font-family:courier;color:orange\">N</span> times in parallel. Each execution of the kernel function `hello` is done by a so-called \"thread block\" or simply \"block\" (we will understand this terminology better later). The set of all thread blocks executing the kernel function is called a \"grid.\" We can programmatically obtain the index of which thread block is currently executing the function using the CUDA-provided runtime variable `blockIdx.x` (which is zero indexed and runs from 0 to N-1). That is, within a kernel we can write:\n",
    "\n",
    "```\n",
    "__global__ void kernel() {\n",
    "\n",
    "    int my_block = blockIdx.x;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "and one execution of the kernel will have `my_block == 0`, another will have `my_block == 1`, etc., all the way up to the instance that has `my_block == N-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's now test what we know about running code in parallel. Edit the file [exercises/hello_with_blocks.cu](exercises/hello_with_blocks.cu) so that when it runs, it prints out the following:\n",
    "\n",
    "```\n",
    "Hello from block: 0\n",
    "Hello from block: 1\n",
    "```\n",
    "\n",
    "(Look for `FIXME` in the code.) \n",
    "\n",
    "Note that the ordering of the above lines may vary; ordering differences do not indicate an incorrect result.\n",
    "\n",
    "Once you've updated the code, compile and run it and make sure your output matches what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o hello_with_blocks exercises/hello_with_blocks.cu; ./hello_with_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, look at [solutions/hello_with_blocks.cu](solutions/hello_with_blocks.cu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Threads\n",
    "\n",
    "A thread block can be subdivided into parallel **threads**. If we change the *second* parameter in the execution configuration, this determines how many threads each thread block is subdivided into. That is, by taking\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">hello<<<N,<span style=\"color:green\">1</span>>>();</span>\n",
    "\n",
    "and turning it into\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">hello<<<N,<span style=\"color:green\">M</span>>>>();</span>\n",
    "\n",
    "the kernel `hello()` will now be launched with `N` thread blocks, each of which has `M` threads. Each thread gets its own invocation of the kernel, so the total number of invocations of the kernel body is now `N * M`. The new syntax we can use to identify which thread is currently executing is `threadIdx.x`:\n",
    "\n",
    "```\n",
    "__global__ void kernel() {\n",
    "\n",
    "    int my_block = blockIdx.x;\n",
    "    int my_thread_in_block = threadIdx.x;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "We'll understand later why this two-level hierarchy exists. For now, we accept it as an aspect of the programming model that we must be comfortable with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's practice with threads. Edit the file [exercises/hello_with_threads.cu](exercises/hello_with_threads.cu) so that when it runs, it prints out the following:\n",
    "\n",
    "```\n",
    "Hello from block: 0, thread: 0\n",
    "Hello from block: 0, thread: 1\n",
    "Hello from block: 1, thread: 0\n",
    "Hello from block: 1, thread: 1\n",
    "```\n",
    "\n",
    "(Look for `FIXME` in the code.) \n",
    "\n",
    "As before, the ordering of the above lines may vary; ordering differences do not indicate an incorrect result.\n",
    "\n",
    "Once you've updated the code, compile and run it and make sure your output matches what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o hello_with_threads exercises/hello_with_threads.cu; ./hello_with_threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, look at [solutions/hello_with_threads.cu](solutions/hello_with_threads.cu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Addition on the Device\n",
    "\n",
    "Now let's look at an example of a kernel that does parallel vector addition on arrays, $c = a + b$. We'll call it `add()` and it might look like:\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "<span style=\"color:green\">__global__</span> void add(int* a, int* b, int* c) {<br />\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; c[<span style=\"color:orange\">blockIdx.x</span>] = a[<span style=\"color:orange\">blockIdx.x</span>] + b[<span style=\"color:orange\">blockIdx.x</span>];<br />\n",
    "}<br /><br />\n",
    "</span>\n",
    "\n",
    "\n",
    "When we call this kernel from `main()`, we would use\n",
    "\n",
    "```\n",
    "    add<<<N, 1>>>(a, b, c);\n",
    "```\n",
    "\n",
    "By using **blockIdx.x** to index into the array, each block handles a different index of the arrays (and that block handles the same index for all three arrays).\n",
    "\n",
    "Separately, we might want to also implement it with only threads:\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "<span style=\"color:green\">__global__</span> void add(int* a, int* b, int* c) {<br />\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; c[<span style=\"color:orange\">threadIdx.x</span>] = a[<span style=\"color:orange\">threadIdx.x</span>] + b[<span style=\"color:orange\">threadIdx.x</span>];<br />\n",
    "}<br /><br />\n",
    "</span>\n",
    "\n",
    "When we call this kernel from `main()`, we would use\n",
    "\n",
    "```\n",
    "    add<<<1, M>>>(a, b, c);\n",
    "```\n",
    "\n",
    "By using **threadIdx.x** to index into the array, every thread in the block handles a different index of the arrays (and that thread handles the same index for all three arrays).\n",
    "\n",
    "If there's two ways to do this, which is preferred? It turns out that the best approach is to *combine* blocks and threads to solve problems. (We'll understand this better when we learn more about the GPU architecture.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Blocks and Threads\n",
    "\n",
    "We’ve seen parallel vector addition using:\n",
    "- Many blocks with one thread each\n",
    "- One block with many threads\n",
    "\n",
    "Let’s adapt vector addition to use both blocks and threads, with a focus on how to index the data.\n",
    "\n",
    "### Indexing Arrays with Blocks and Threads\n",
    "\n",
    "This is no longer as simple as using <span style=\"font-family:courier;\">**blockIdx.x**</span> and <span style=\"font-family:courier;\">**threadIdx.x**</span>. Consider indexing an array with one element per thread (8 threads/block):\n",
    "\n",
    "![](images/block_and_thread_indexing.png)\n",
    "\n",
    "With `M` threads/block a unique index for each thread is given by:\n",
    "\n",
    "```\n",
    "int index = threadIdx.x + blockIdx.x * M;\n",
    "```\n",
    "\n",
    "### Indexing Arrays: Example\n",
    "\n",
    "Which thread will operate on the red element?\n",
    "\n",
    "![](images/indexing_arrays_example.png)\n",
    "\n",
    "### Vector Addition with Blocks and Threads\n",
    "\n",
    "Use the built-in variable <span style=\"font-family:courier;color:orange\">**blockDim.x**</span> to get the number of threads per block.\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "    int index = threadIdx.x + blockIdx.x * <span style=\"color:orange\">blockDim.x</span>;<br /><br />\n",
    "</span>\n",
    "\n",
    "Now we can write a combined version of `add()` to use parallel threads and parallel blocks:\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "<span style=\"color:green\">__global__</span> void add(int* a, int* b, int* c) {<br />\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; int index = threadIdx.x + blockIdx.x * blockDim.x;<br/>\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; c[<span style=\"color:orange\">index</span>] = a[<span style=\"color:orange\">index</span>] + b[<span style=\"color:orange\">index</span>];<br />\n",
    "}<br /><br />\n",
    "</span>\n",
    "\n",
    "and when we call this kernel from `main()` we need to do\n",
    "\n",
    "```\n",
    "    add<<<N / M, M>>>(a, b, c);\n",
    "```\n",
    "\n",
    "where `M` is the number of threads per block we choose, and `N` is the length of the arrays. `N / M` then ensures we launch as many threads as there are elements in the array (assuming it is an integer multiple of `M`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's practice with combined thread and block indexing. Edit the file [exercises/hello_with_a_specific_thread.cu](exercises/hello_with_a_specific_thread.cu) so that when it runs, it prints out *only* (and *exactly once*) the following:\n",
    "\n",
    "```\n",
    "Hello from unique thread index: 773\n",
    "```\n",
    "\n",
    "(Look for `FIXME` in the code.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o hello_with_a_specific_thread exercises/hello_with_a_specific_thread.cu; ./hello_with_a_specific_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get stuck, look at [solutions/hello_with_a_specific_thread.cu](solutions/hello_with_a_specific_thread.cu). Note that this solution is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Arbitrary Vector Sizes\n",
    "\n",
    "By now, we can see that the total number of threads in a kernel launch is an integer multiple of the number of threads in a block, but typical problems are not friendly multiples of <span style=\"font-family:courier;\">**blockDim.x**</span>. We must avoid accessing beyond the end of the arrays:\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "<span style=\"color:green\">__global__</span> void add(int* a, int* b, int* c, int N) {<br />\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; int index = threadIdx.x + blockIdx.x * blockDim.x;<br/>\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; if (index < N)<br/>\n",
    "    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; c[<span style=\"color:orange\">index</span>] = a[<span style=\"color:orange\">index</span>] + b[<span style=\"color:orange\">index</span>];<br />\n",
    "}<br /><br />\n",
    "</span>\n",
    "\n",
    "Note that we added the length of the arrays `N` as a kernel parameter. The updated kernel launch looks like:\n",
    "\n",
    "<span style=\"font-family:courier;font-size:large\">\n",
    "    add<<<<span style=\"color:orange\">(N + M - 1) / M</span>, M>>>(a, b, c, N);<br /><br />\n",
    "</span>\n",
    "\n",
    "`(N + M - 1) / M` is a simple trick for doing integer division that rounds upward (to ensure we have at least as many threads as there are elements in the array, with a few threads left over if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can tackle our vector addition example, we have to learn about one more topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Processing Flow\n",
    "\n",
    "A typical accelerated program using CUDA employs the following workflow. We start with execution on the CPU, and copy our input data from the CPU memory (host memory) to the GPU memory (device memory).\n",
    "\n",
    "![](images/simple_processing_flow_1.png)\n",
    "\n",
    "Next, we load up the code for execution on the GPU and run the code on the GPU. The computations are done on the GPU, and the GPU can cache the data from its own memory in local on-chip caches for maximum performance.\n",
    "\n",
    "![](images/simple_processing_flow_2.png)\n",
    "\n",
    "Finally we copy the results back from GPU memory to CPU memory.\n",
    "\n",
    "![](images/simple_processing_flow_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's learn about our tools for memory management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "Host and device memory are separate entities:\n",
    "\n",
    "\n",
    "- **Device** pointers point to GPU memory\n",
    "  - Typically passed to device code\n",
    "  - Typically *not* dereferenced in host code\n",
    "\n",
    "\n",
    "- **Host** pointers point to CPU memory\n",
    "  - Typically not passed to device code\n",
    "  - Typically *not* dereferenced in device code\n",
    "  \n",
    "  \n",
    "There are some special cases we won't cover here, including pinned memory and managed memory, and platforms with coherent CPU/GPU memory accesses such as IBM Power9 servers.\n",
    "\n",
    "There is a simple CUDA API for handling device memory.\n",
    "\n",
    "- `cudaMalloc()`, `cudaFree()`, `cudaMemcpy()`\n",
    "\n",
    "These are analogous to standard CPU memory management functions in C\n",
    "\n",
    "- `malloc()`, `free()`, `memcpy()`\n",
    "\n",
    "The syntax of the allocation and deallocation APIs looks like:\n",
    "\n",
    "```\n",
    "    cudaMalloc(&ptr, size_in_bytes_to_allocate);\n",
    "    cudaFree(ptr);\n",
    "```\n",
    "\n",
    "So `cudaFree` looks like `free`, but `cudaMalloc` is a little different from `malloc` in that it takes the address of the pointer and modifies it directly (this is because, as we're about to see, CUDA APIs generally return error codes so you know whether they worked, so the allocated pointer cannot also be the return value of the API).\n",
    "\n",
    "The syntax of the memcpy API looks like:\n",
    "\n",
    "```\n",
    "    cudaMemcpy(destination_ptr, source_ptr, size_in_bytes_to_copy, direction);\n",
    "```\n",
    "\n",
    "where `direction` can be `cudaMemcpyHostToDevice` (for copying data from CPU to GPU) or `cudaMemcpyDeviceToHost` (for copying data from GPU to CPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's practice with memory allocation. In [exercises/memory_management.cu](exercises/memory_management.cu), allocate data for a single integer, set it to an initial value on the host, copy that initial value to the GPU, then launch a kernel to change its value, then copy the new value back to the host and verify that it changed as you expected. Consult [solutions/memory_management.cu](solutions/memory_management.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o memory_management exercises/memory_management.cu; ./memory_management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "Every CUDA runtime API call returns an error code. It's good practice (especially if you're having trouble) to rigorously check these error codes. The error code is type `cudaError_t` (which is an integer). If the value of the error is `cudaSuccess` (0) then the CUDA API call returned without errors. Otherwise some error occurred, and CUDA provides a number of error codes corresponding to different failure conditions. See the [error handling](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html#group__CUDART__ERROR) section of the CUDA Runtime API for more information, and specifically you view the [cudaError enum](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g3f51e3575c2178246db0a94a430e0038) for a list of the different error codes.\n",
    "\n",
    "To use this in code, it's as simple as\n",
    "\n",
    "```\n",
    "    int* a;\n",
    "    // Illegal: cannot allocate a negative number of bytes\n",
    "    cudaError_t err = cudaMalloc(&a, -1);\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error %d\\n\", err);\n",
    "        exit(-1);\n",
    "    }\n",
    "```\n",
    "\n",
    "Since you won't have the error codes memorized by heart, CUDA provides a convenience function `cudaGetErrorString` for returning a human-readable string corresponding to a given error code.\n",
    "\n",
    "```\n",
    "    int* a;\n",
    "    // Illegal: cannot allocate a negative number of bytes\n",
    "    cudaError_t err = cudaMalloc(&a, -1);\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error %s\\n\", cudaGetErrorString(err));\n",
    "        exit(-1);\n",
    "    }\n",
    "```\n",
    "\n",
    "One additional issue to handle is that `__global__` functions have void return type; how do we get errors that occur in kernels? These errors can come in two flavors. One class of errors is errors in the kernel launch API (the triple chevron syntax is just syntactic sugar that maps to an actual CUDA runtime API call, `cudaLaunchKernel`). To detect any errors in the kernel launch (that is, an invalid parameter), we can use the API call `cudaGetLastError()` which returns the error code for whatever the last CUDA API call was. The second class of error is an error that occurs asynchronously during the kernel launch. `cudaGetLastError()` called immediately after the kernel will not generally pick this up (the error may not have even happened by the time it checks). But fortunately, if an asynchronous error has been detected, the next CUDA runtime API will catch it. So if we insert a `cudaDeviceSynchronize()` after the kernel, this call will return any errors associated with the kernel launch. \n",
    "\n",
    "```\n",
    "    // Illegal: number of blocks cannot be negative\n",
    "    kernel<<<-1,1>>>();\n",
    "    cudaError_t err = cudaGetLastError();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error %s\\n\", cudaGetErrorString(err));\n",
    "        exit(-1);\n",
    "    }\n",
    "    \n",
    "    err = cudaDeviceSynchronize();\n",
    "    if (err != cudaSuccess) {\n",
    "        printf(\"CUDA error %s\\n\", cudaGetErrorString(err));\n",
    "        exit(-1);\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "The code [exercises/error_handling.cu](exercises/error_handling.cu) intentionally has several mistakes. Try locating the errors yourself by inspection, then before fixing them insert CUDA error checking after all the CUDA runtime API calls and see if the results match your expectation. Then fix the errors one by one until the code runs to completion without errors. An example solution is provided in [solutions/error_handling.cu](solutions/error_handling.cu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o error_handling exercises/error_handling.cu; ./error_handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Now let's pull together all the skills we've learned in this lab. We've given you a skeleton version of the vector addition program in [exercises/vector_addition.cu](exercises/vector_addition.cu). Edit the code to build a complete vector_add program. (Or, if you're up for a challenge, see if you can write the complete vector addition program from scratch.) Compile it and run it. You can refer to [solutions/vector_addition.cu](solutions/vector_addition.cu) for a complete example.\n",
    "\n",
    "Note that this skeleton code includes a macro for CUDA error checking to make your life easier.\n",
    "\n",
    "Typical output when complete would look like this:\n",
    "\n",
    "```\n",
    "A[0] = 0.840188\n",
    "B[0] = 0.394383\n",
    "C[0] = 1.234571\n",
    "```\n",
    "\n",
    "The important thing is that `C[0] = A[0] + B[0]` if you've obtained correct execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o vector_addition exercises/vector_addition.cu; ./vector_addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "In this lab we have learned:\n",
    "\n",
    "- The difference between host (CPU) and device (GPU)\n",
    "\n",
    "\n",
    "- Using `__global__` to declare a function as device code\n",
    "  - Executes on the device\n",
    "  - Called from the host (or possibly from other device code)\n",
    "\n",
    "\n",
    "- Passing parameters from host code to a device function\n",
    "\n",
    " \n",
    "- How to compile and run CUDA code\n",
    "\n",
    "\n",
    "- Basic memory management (`cudaMalloc()`, `cudaFree()`, `cudaMemcpy`)\n",
    "\n",
    "\n",
    "- Launching parallel kernels that use both blocks and threads\n",
    "  - Launch `N` copies of `add()` with `add<<<N,1>>>(...);`\n",
    "  - Use `blockIdx.x` to access block index\n",
    "  - Use `threadIdx.x` to access thread index within block\n",
    "  - Assign elements to threads: `int index = threadIdx.x + blockIdx.x * blockDim.x;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Study\n",
    "\n",
    "[An introduction to CUDA](https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/)\n",
    "\n",
    "[Another introduction to CUDA](https://devblogs.nvidia.com/even-easier-introduction-cuda/)\n",
    "\n",
    "[CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "\n",
    "[CUDA Runtime API Documentation](https://docs.nvidia.com/cuda/index.htmlhttps://docs.nvidia.com/cuda/cuda-runtime-api/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "A skeleton code for a (naive) matrix multiply is given to you in [exercises/matrix_mul.cu](exercises/matrix_mul.cu). See if you can complete it to get a correct result. If you need help, you can refer to [solutions/matrix_mul.cu](solutions/matrix_mul.cu).\n",
    "\n",
    "This example introduces 2D threadblock/grid indexing, something we did not previously cover. If you study the code you will probably be able to see how it is a structural extension from the 1D case.\n",
    "\n",
    "This code includes built-in error checking, so a correct result is indicated by the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -arch=native -o matrix_mul exercises/matrix_mul.cu; ./matrix_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Materials\n",
    "\n",
    "You can download this notebook using the `File > Download as > Notebook (.ipnyb)` menu item. Source code files can be downloaded from the `File > Download` menu item after opening them."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
