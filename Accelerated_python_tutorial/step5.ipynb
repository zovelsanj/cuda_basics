{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a339e968",
   "metadata": {},
   "source": [
    "## Step 5: Multi-Node Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42555f-3915-47d8-b17d-174725ab000e",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to use Nsight Systems in a multi-node performance analysis.\n",
    "\n",
    "## 5.1 Data Collection\n",
    "Nsight Systems is not aware of any cluster schedulers such as Slurm. Thus, `nsys profile` should be put after the scheduler's run command, usually directly before the application, e.g.\n",
    "\n",
    "`srun <srun args> nsys profile <nsys args> -o report_name.%q{SLURM_PROCID} your_application`\n",
    "\n",
    "This will result in one report file per process or rank.\n",
    "\n",
    "When data is collected for an increasing number of processes or ranks, it is reasonable to limit the recorded data. Otherwise the Nsight Systems GUI or the recipe analysis might not be able to handle the sheer amount of data. There are several ways to limit data recording:\n",
    "\n",
    "- Capture ranges (`--capture-range`) are used to limit the interval in which data is collected. Capture ranges can be triggered via the CUDA profiler API and NVTX events.\n",
    "- Set a collection duration (`--duration`).\n",
    "- Delay data recording (`--delay`), e.g. to skip the initialization phase.\n",
    "- NVTX domain filtering can include or exclude events from an NVTX domain (`--nvtx-domain-include` and `--nvtx-domain-exclude`).\n",
    "- Record only a set of ranks or just a single one, e.g. with helper script\n",
    "\n",
    "```bash\n",
    "    #/bin/bash\n",
    "    if [ $SLURM_LOCALID -eq 0 ]; then\n",
    "      nsys profile \"$@\"\n",
    "    else\n",
    "      \"$@\"\n",
    "    fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec3f1f",
   "metadata": {},
   "source": [
    "## 5.2 Case Study: MLPerf\n",
    "\n",
    "For this case study, we are using the MLPerf DeepCAM benchmark, which trains a deep learning segmentation model for identifying extreme weather phenomena in climate simulation data.\n",
    "MLPerf DeepCAM uses DALI (NVIDIA Data Loading Library) to load the training data and NCCL (NVIDIA Collective Communication Library) to exchange data between GPUs.\n",
    "\n",
    "Since we do not have access to a multi-node cluster in this lab, the reports have been precollected in the folder *reports/precollected/mlperf*, for an initial run and after an optimization step.\n",
    "The profiling runs were executed on two nodes of EOS, an NVIDIA DGX SuperPOD cluster with 8 H100 GPUs per node.\n",
    "The following commands have been used to run the benchmark code:\n",
    "```bash\n",
    "    # Apply system-wide options only to one rank per node\n",
    "    if [ $SLURM_LOCALID -eq 0 ]; then\n",
    "        nsys_flags_local0=\"--nic-metrics=true --storage-metrics=--storage-devices=all\"\n",
    "    fi\n",
    "\n",
    "    srun -n 16 <more srun arguments> \\\n",
    "      nsys profile ${nsys_flags_local0} --trace=cuda,nvtx,osrt \\\n",
    "        --cuda-graph-trace=node \\\n",
    "        --capture-range=cudaProfilerApi --kill=none \\\n",
    "        -o ${OUTPUT_DIR}/mlperf_...${SLURM_PROCID} -f true \\\n",
    "        /usr/bin/python ./train.py --capture_range_start 11 --capture_range_stop 29 <more training parameters>\n",
    "```\n",
    "\n",
    "The command runs 16 ranks/processes across the two nodes.\n",
    "The `--cuda-graph-trace=node` instructs to record at CUDA kernel granularity, which enables us to distinguish between NCCL and CUDA compute kernels. Otherwise, only the begin and end of the graph execution is recorded.\n",
    "The `--capture-range=cudaProfilerApi` flag limits the recording to a specific range. The application code has to call the CUDA profiler APIs to make this work.\n",
    "The `kill=none` flag instructs Nsight Systems to not send a signal to the application, when the profiling session ends.\n",
    "System-wide Nsight Systems CLI flags, e.g. for network interface card (NIC) and storage metrics sampling, are only set for the node-local rank 0 to avoid collecting redundant data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65f726",
   "metadata": {},
   "source": [
    "## 5.3 Report Inspection\n",
    "\n",
    "The report files of the initial run are located in the folder *reports/precollected/mlperf/initial_run*.\n",
    "It contains 16 report files, one for each rank.\n",
    "Let's take a look at the report of rank 0 in the Nsight Systems GUI to get an idea on the code execution.\n",
    "The file name is *mlperf2_bs8_nvme-posix_t1_steps11-29.00.nsys-rep*.\n",
    "\n",
    "<center><img src=images/step5/mlperf_initial_run_nsys_timeline.png width=95%></center>\n",
    "\n",
    "Some observations:\n",
    "- Continuously reading (training data) with a throughput of about 14 GB/s.\n",
    "- Lots of processes/threads are often in *pthread_cond_wait* state.\n",
    "- One process is performing the file reads via DALI, which results in many `fopen` and `fread` operations.\n",
    "- Step duration increases over time from about 100ms to 220ms. We ignore the first step (11) at the beginning of our capture range, since it likely includes some profiling startup overhead.\n",
    "- `ncclAllReduce` duration varies quite a bit in each step.\n",
    "- `ncclDevKernel_AllReduce_Sum_f32_RING` is the kernel with the highest execution percentage (about 24%).\n",
    "- GPU idle time before the NCCL kernel `ncclDevKernel_AllReduce_Sum_f32_RING` increases over time.\n",
    "\n",
    "Given these observations, the NCCL allreduce operation seems to wait for training data to be loaded from disk.\n",
    "\n",
    "So far, we have only looked at a single report file, representing the execution of only a single rank and GPU.\n",
    "There are several Nsight Systems recipes that can help us better understand the overall performance and identify inefficiencies in this multi-node program run.\n",
    "To analyze the IO behavior over all disks and nodes, there is the **storage_util_map** recipe.\n",
    "For applications that use NCCL, the **nccl_gpu_time_util_map** provides an overview on the temporal GPU utilization by NCCL and compute kernels as well as their overlap.\n",
    "\n",
    "Since NCCL kernels are dominating the GPU usage on rank 0, let us investigate whether this is the case for all ranks.\n",
    "Execute the following code cell to run the *NCCL GPU Time Utilization Heatmap* recipe on the reports of the initial MLPerf run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ee66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nccl_gpu_time_util_map [-h] [--output OUTPUT] [--force-overwrite]\n",
      "                              --input INPUT [INPUT ...] [--bins BINS]\n",
      "                              [--disable-alignment]\n",
      "                              [--filter-time [start_time]/[end_time] |\n",
      "                              --filter-nvtx range[@domain][/index]]\n",
      "                              [--mode {none,concurrent,dask-futures}]\n",
      "nccl_gpu_time_util_map: error: argument --input: reports/precollected/mlperf/initial_run does not exist.\n"
     ]
    }
   ],
   "source": [
    "!nsys recipe nccl_gpu_time_util_map \\\n",
    "--output reports/precollected/mlperf/initial_run/results_nccl_gpu_util_map \\\n",
    "--force-overwrite \\\n",
    "--bins 100 \\\n",
    "--log-level=error \\\n",
    "--input reports/precollected/mlperf/initial_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac8708",
   "metadata": {},
   "source": [
    "After the recipe execution is finished, we can open the Jupyter notebook [reports/precollected/mlperf/initial_run/results_nccl_gpu_util_map/heatmap.ipynb](reports/precollected/mlperf/initial_run/results_nccl_gpu_util_map/heatmap.ipynb) and run all kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba0ae6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Screenshots and observations:\n",
    "\n",
    "<center><img src=images/step5/mlperf_initial_run_nccl_recipe_util_sum.png width=95%></center>\n",
    "<center><img src=images/step5/mlperf_initial_run_nccl_recipe_heatmap_util.png width=90%></center>\n",
    "\n",
    "The NCCL GPU *Utilization Summary* graph shows *NCCL* and *Compute* kernels alternating with no overlap.\n",
    "The *Overlap* heatmap confirms the latter.\n",
    "The *Utilization All* heatmap shows the overall GPU utilization per rank.\n",
    "\n",
    "In general, the exection pattern represented by the heatmaps has some irregularities, which indicates inefficiencies.\n",
    "A regular pattern is usually also better in terms of performance.\n",
    "For parallel applications running in SPMD or a lock-step-like mode, time-aligned execution phases are ideal.\n",
    "Otherwise, it is likely that synchronization between the ranks will cause waiting times.\n",
    "\n",
    "## 5.4 Optimize Data Loading\n",
    "\n",
    "A fundamental bottleneck in the initial execution is data loading from disks, which cannot cover the processing capacities of the GPUs.\n",
    "Different data loading times ultimately also result in the irregular execution pattern.\n",
    "One if the issues is that the kernel caches the data reading, which is not efficient for the large amount of data that is read only once. Using *O_DIRECT* for file reading improves the situation. Furthermore, it can be beneficial to use more threads for data loading. Hence, we used *O_DIRECT* and four instead of one thread for loading the training data in another execution of our MLPerf benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e79cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Exercise</b>\n",
    "\n",
    "<p>Execute the <i>NCCL GPU Time Utilization Heatmap</i> recipe for the profiles in <i>reports/precollected/mlperf/optimized_run</i>.\n",
    "<br>What does the pattern look like?\n",
    "<br>How is the GPU utilized by NCCL and compute kernels?</p>\n",
    "\n",
    "<p>Inspect the report of rank 0 <i>mlperf2_bs8_nvme-odirect_t4_steps11-29.00.nsys-rep</i> using the Nsight Systems GUI for more in-depth analyses.\n",
    "<br>How is the IO data reading throughput?\n",
    "<br>What about the step duration?\n",
    "<br>What happend to the DALI row/ranges?</p>\n",
    "\n",
    "<p> You can use the following empty code cell to execute the recipe.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf857735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc44ffe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nccl_gpu_time_util_map [-h] [--output OUTPUT] [--force-overwrite]\n",
      "                              --input INPUT [INPUT ...] [--bins BINS]\n",
      "                              [--disable-alignment]\n",
      "                              [--filter-time [start_time]/[end_time] |\n",
      "                              --filter-nvtx range[@domain][/index]]\n",
      "                              [--mode {none,concurrent,dask-futures}]\n",
      "nccl_gpu_time_util_map: error: argument --input: /dli/task/reports/precollected/mlperf/optimized_run does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Execute the NCCL GPU Time Utilization Heatmap recipe.\n",
    "!nsys recipe nccl_gpu_time_util_map \\\n",
    "--output reports/precollected/mlperf/optimized_run/results_nccl_gpu_util_map \\\n",
    "--force-overwrite \\\n",
    "--bins 100 \\\n",
    "--log-level=error \\\n",
    "--input reports/precollected/mlperf/optimized_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f8767",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Open the **NCCL GPU utilization** notebook [results_nccl_gpu_util_map/heatmap.ipynb](reports/precollected/mlperf/optimized_run/results_nccl_gpu_util_map/heatmap.ipynb) and run all kernels.\n",
    "\n",
    "<center><img src=images/step5/mlperf_optimized_run_nccl_recipe_util_sum.png width=95%></center>\n",
    "<center><img src=images/step5/mlperf_optimized_run_nccl_recipe_heatmap_util.png width=90%></center>\n",
    "\n",
    "Observations:\n",
    "- NCCL communication kernels are much shorter, but they still do not overlap with GPU computation.\n",
    "- The execution pattern is much more aligned across the ranks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5808d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The following screenshot shows the Nsight Systems GUI for the report of rank 0 *mlperf2_bs8_nvme-odirect_t4_steps11-29.00.nsys-rep*.\n",
    "\n",
    "<center><img src=images/step5/mlperf_optimized_run_nsys_timeline.png width=95%></center>\n",
    "\n",
    "The Nsight Systems timeline visualization of rank 0 confirms the observations from the recipe results.\n",
    "It also shows that the step duration is almost constant now and only about half of the execution time than before.\n",
    "\n",
    "Data is not read continuously any more, but in intervals with about three times the throughput than before.\n",
    "Therefore, data reading is likely not the bottleneck any more. The *storage utilization* recipe would confirm this.\n",
    "Feel free to try it on both, the initial and the optimized run and compare the results.\n",
    "\n",
    "Still having GPU gaps (about 1ms) and NCCL device kernels that do not overlap with GPU computation indicate that there is optimization potential from a high-level execution perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969bb8",
   "metadata": {},
   "source": [
    "### 5.5 Summary\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <p>We investigated a bottleneck in an MLPerf multi-node execution using the Nsight Systems timeline and two Nsight Systems recipes.</p>\n",
    "    <p>\n",
    "        NCCL communication is performed via CUDA kernels and shown as compute kernels in the Nsight Systems timeline.<br>\n",
    "        The NCCL recipes can help to expose overlap of NCCL communication and GPU computation.\n",
    "    </p>\n",
    "    <p>Consider running recipes on the reports of a parallel program to get an overview on the parallel execution before examining individual reports in the Nsight Systems GUI.</p>\n",
    "</div>\n",
    "\n",
    "Please click [here](summary.ipynb) to move to the end of the Nsight Systems lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
