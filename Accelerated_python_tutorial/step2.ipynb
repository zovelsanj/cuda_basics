{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23efc05e",
   "metadata": {},
   "source": [
    "## Step 2: Data Transfers between Host and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e404f36-e2db-45e8-9695-1bf0c744b445",
   "metadata": {},
   "source": [
    "Any communication between the host and GPU devices usually happens on a PCIe link which is very slow, so it is important that we optimize any data transfers between the host and the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30232954-5b99-49a7-abe7-ca6f99d1ff57",
   "metadata": {},
   "source": [
    "## 2.1 Analyze the Profile\n",
    "\n",
    "Use the Nsight Systems GUI to analyze the profile that we generated at the end of the last notebook for the code [main_nvtx-cvcuda.py](video_segmentation/main_nvtx-cvcuda.py). If you closed the browser tab with the Nsight Streamer, go back to the [step 1](step1.ipynb) notebook and click the link generated in the last code cell of section 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b699e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The screenshot below shows the timeline view, zoomed into one batch in the pipeline.\n",
    "\n",
    "<center><img src=images/step2/nsys_timeline_memcpy_merged.png></center>\n",
    "\n",
    "Some observations:\n",
    "1. If you select the *Memory* timeline row and right-click to select the *Show in Events View* option, you can see the list of memory operations in the *Events View*, sorted according to their start time. Select the _pipeline_ NVTX range and right-click to select the *Apply Filter* option. This will filter the *Events View* to show only those events that occurred within the _pipeline_ NVTX range. You will see a list of alternating *Memcpy HtoD* and *Memcpy DtoH* operations.\n",
    "2. The data is being copied from host to device before any of the CUDA kernels for the batch are executed on the GPU. This is because the first step in the algorithm, decoding, is still being done on the CPU.\n",
    "3. The data is being copied out from device to host after the CUDA kernels for a batch finish executing on the GPU. This is because the final encoding step for the batch is still being done on the CPU.\n",
    "4. The CudaMemcpyAsync calls are actually blocking the CPU thread until the data is transferred to/from the GPU. (For advanced CUDA users, it is because pageable memory is being used. See the *CUDA Async Memcpy with Pageable Memory* rule in *Expert Systems View* for the explanation and how to address it. This will not be covered by the instructor.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919e500",
   "metadata": {},
   "source": [
    "## 2.2 Optimize Code to Address the Bottleneck\n",
    "If we can move the decoding and encoding steps of the algorithm to the GPU as well and keep the data on the GPU until the full pipeline is complete, that would help us avoid the memory transfers.\n",
    "\n",
    "NVIDIA GPUs contain one or more hardware-based decoder and encoder(s) (separate from the CUDA cores) which provides fully-accelerated hardware-based video decoding and encoding for several popular codecs. With decoding/encoding offloaded, the graphics engine and the CPU are free for other operations.\n",
    "\n",
    "<center><img src=images/Nvenc_dec.JPG></center>\n",
    "\n",
    "[NVIDIAâ€™s Video Codec SDK](https://developer.nvidia.com/video-codec-sdk) offers hardware-accelerated video encoding and decoding through highly optimized C/C++ APIs.\n",
    "Video encoding and decoding is useful for a wide range of users, including computer vision experts, researchers and Deep Learning developers.\n",
    "[PyNvVideoCodec](https://docs.nvidia.com/video-technologies/pynvvideocodec) provides Python bindings for harnessing such video encoding and decoding capabilities when working with videos in Python.\n",
    "\n",
    "Execute the cell below to see the code changes in the main Python program needed for the optimization. It shows the diff between the [main_nvtx-cvcuda.py](video_segmentation/main_nvtx-cvcuda.py) and [main_nvtx-cvcuda-nvcodec.py](video_segmentation/main_nvtx-cvcuda-nvcodec.py) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e460c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m--- video_segmentation/main_nvtx-cvcuda.py\t2025-05-28 17:17:45.284432419 +0100\u001b[0m\n",
      "\u001b[1m+++ video_segmentation/main_nvtx-cvcuda-nvcodec.py\t2025-05-28 17:17:44.696452219 +0100\u001b[0m\n",
      "\u001b[36m@@ -22,3 +22,3 @@\u001b[0m\n",
      " # Select codec backend ---------------------------------\n",
      "\u001b[31m-from opencv_utils import BatchEncoder, BatchDecoder\u001b[0m\n",
      "\u001b[32m+from nvcodec_utils import BatchEncoder, BatchDecoder\u001b[0m\n",
      " \n"
     ]
    }
   ],
   "source": [
    "!diff -U1 -d --color=always video_segmentation/main_nvtx-cvcuda.py video_segmentation/main_nvtx-cvcuda-nvcodec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cc117",
   "metadata": {},
   "source": [
    "## 2.3 Profile to Verify the Optimization\n",
    "\n",
    "So far we have used the CLI to profile the application. Another option is NVIDIA's Jupyterlab extension which enables profiling of code cells directly. See https://pypi.org/project/jupyterlab-nvidia-nsight/\n",
    "The extension is pre-installed in this Jupyterlab notebook. Let's use it to profile the optimized code.\n",
    "\n",
    "<img src=images/step2/jupyterlab-nvidia-nsight-extension.png>\n",
    "\n",
    "The following code cell has a simple Python command to run the optimized code. To profile it, use the following instructions:\n",
    "- Click on the **NVIDIA Nsight** menu option\n",
    "- Select the **Profiling with Nsight Systems...** option\n",
    "- Set the _nsys launch_ command options to `--trace=cuda,nvtx,osrt,nvvideo`. These are the same options as used in the previous notebook with the addition of the *nvvideo* trace option which will make Nsight Systems trace the NVIDIA Video Codec API calls.\n",
    "\n",
    "<img src=images/extension_defaults_change.jpg>\n",
    "\n",
    "- Hit _Restart_ to restart the kernel\n",
    "- Click on the code cell to profile and from the NVIDIA Nsight menu select the **Run and profile selected cells...** option (green arrow in the toolbar). You will see a popup to _Set nsys command options_, which you can leave blank to use the default and click _OK_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a863822f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<NV_ENC_CAPS.NUM_MAX_BFRAMES: 0>: 4, <NV_ENC_CAPS.SUPPORTED_RATECONTROL_MODES: 1>: 63, <NV_ENC_CAPS.SUPPORT_FIELD_ENCODING: 2>: 0, <NV_ENC_CAPS.SUPPORT_MONOCHROME: 3>: 0, <NV_ENC_CAPS.SUPPORT_FMO: 4>: 0, <NV_ENC_CAPS.SUPPORT_QPELMV: 5>: 1, <NV_ENC_CAPS.SUPPORT_BDIRECT_MODE: 6>: 1, <NV_ENC_CAPS.SUPPORT_CABAC: 7>: 1, <NV_ENC_CAPS.SUPPORT_ADAPTIVE_TRANSFORM: 8>: 1, <NV_ENC_CAPS.SUPPORT_STEREO_MVC: 9>: 1, <NV_ENC_CAPS.NUM_MAX_TEMPORAL_LAYERS: 10>: 4, <NV_ENC_CAPS.SUPPORT_HIERARCHICAL_PFRAMES: 11>: 1, <NV_ENC_CAPS.SUPPORT_HIERARCHICAL_BFRAMES: 12>: 1, <NV_ENC_CAPS.LEVEL_MAX: 13>: 62, <NV_ENC_CAPS.LEVEL_MIN: 14>: 10, <NV_ENC_CAPS.SEPARATE_COLOUR_PLANE: 15>: 1, <NV_ENC_CAPS.WIDTH_MAX: 16>: 4096, <NV_ENC_CAPS.HEIGHT_MAX: 17>: 4096, <NV_ENC_CAPS.SUPPORT_TEMPORAL_SVC: 18>: 1, <NV_ENC_CAPS.SUPPORT_DYN_RES_CHANGE: 19>: 1, <NV_ENC_CAPS.SUPPORT_DYN_BITRATE_CHANGE: 20>: 1, <NV_ENC_CAPS.SUPPORT_DYN_FORCE_CONSTQP: 21>: 1, <NV_ENC_CAPS.SUPPORT_DYN_RCMODE_CHANGE: 22>: 0, <NV_ENC_CAPS.SUPPORT_SUBFRAME_READBACK: 23>: 1, <NV_ENC_CAPS.SUPPORT_CONSTRAINED_ENCODING: 24>: 1, <NV_ENC_CAPS.SUPPORT_INTRA_REFRESH: 25>: 1, <NV_ENC_CAPS.SUPPORT_CUSTOM_VBV_BUF_SIZE: 26>: 1, <NV_ENC_CAPS.SUPPORT_DYNAMIC_SLICE_MODE: 27>: 1, <NV_ENC_CAPS.SUPPORT_REF_PIC_INVALIDATION: 28>: 1, <NV_ENC_CAPS.PREPROC_SUPPORT: 29>: 0, <NV_ENC_CAPS.ASYNC_ENCODE_SUPPORT: 30>: 0, <NV_ENC_CAPS.MB_NUM_MAX: 31>: 65536, <NV_ENC_CAPS.MB_PER_SEC_MAX: 32>: 983040, <NV_ENC_CAPS.SUPPORT_YUV444_ENCODE: 33>: 1, <NV_ENC_CAPS.SUPPORT_LOSSLESS_ENCODE: 34>: 1, <NV_ENC_CAPS.SUPPORT_SAO: 35>: 0, <NV_ENC_CAPS.SUPPORT_MEONLY_MODE: 36>: 1, <NV_ENC_CAPS.SUPPORT_LOOKAHEAD: 37>: 1, <NV_ENC_CAPS.SUPPORT_TEMPORAL_AQ: 38>: 1, <NV_ENC_CAPS.SUPPORT_10BIT_ENCODE: 39>: 0, <NV_ENC_CAPS.NUM_MAX_LTR_FRAMES: 40>: 8, <NV_ENC_CAPS.SUPPORT_WEIGHTED_PREDICTION: 41>: 1, <NV_ENC_CAPS.DYNAMIC_QUERY_ENCODER_CAPACITY: 42>: 100, <NV_ENC_CAPS.SUPPORT_BFRAME_REF_MODE: 43>: 3, <NV_ENC_CAPS.SUPPORT_EMPHASIS_LEVEL_MAP: 44>: 1, <NV_ENC_CAPS.WIDTH_MIN: 45>: 145, <NV_ENC_CAPS.HEIGHT_MIN: 46>: 49, <NV_ENC_CAPS.SUPPORT_MULTIPLE_REF_FRAMES: 47>: 1, <NV_ENC_CAPS.SUPPORT_ALPHA_LAYER_ENCODING: 48>: 0, <NV_ENC_CAPS.???: 49>: 1, <NV_ENC_CAPS.???: 50>: 1}\n",
      "119 batches processed\n"
     ]
    }
   ],
   "source": [
    "!python video_segmentation/main_nvtx-cvcuda-nvcodec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a88fe-5388-4157-a014-d320c123bcda",
   "metadata": {},
   "source": [
    "\n",
    "Once the profiling is done, you will see a popup notifying you when the report file is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3105e8d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b> Optionally enable Python sampling</b>\n",
    "\n",
    "<p>Look up the relevant CLI flags in the <a href=\"https://docs.nvidia.com/nsight-systems/UserGuide/index.html#python-profiling\">Nsight Systems User Guide</a> or use `nsys profile --help`.</p>\n",
    "\n",
    "Use the Jupyterlab Nsight extension or the <i>nsys profile</i> command to collect a profile for the <i>video_segmentation/main_nvtx-cvcuda-nvcodec.py</i> program with Python sampling enabled.<br>\n",
    "Optionally set the Python sampling frequency to 400Hz.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d10d9d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Device-side CUDA Event completion trace is currently enabled.\n",
      "         This may increase runtime overhead and the likelihood of false\n",
      "         dependencies across CUDA Streams. If you wish to avoid this, please\n",
      "         disable the feature with --cuda-event-trace=false.\n",
      "WARNING: CPU IP/backtrace sampling not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "WARNING: CPU context switch tracing not supported, disabling.\n",
      "Try the 'nsys status --environment' command to learn more.\n",
      "\n",
      "Collecting data...\n",
      "{<NV_ENC_CAPS.NUM_MAX_BFRAMES: 0>: 4, <NV_ENC_CAPS.SUPPORTED_RATECONTROL_MODES: 1>: 63, <NV_ENC_CAPS.SUPPORT_FIELD_ENCODING: 2>: 0, <NV_ENC_CAPS.SUPPORT_MONOCHROME: 3>: 0, <NV_ENC_CAPS.SUPPORT_FMO: 4>: 0, <NV_ENC_CAPS.SUPPORT_QPELMV: 5>: 1, <NV_ENC_CAPS.SUPPORT_BDIRECT_MODE: 6>: 1, <NV_ENC_CAPS.SUPPORT_CABAC: 7>: 1, <NV_ENC_CAPS.SUPPORT_ADAPTIVE_TRANSFORM: 8>: 1, <NV_ENC_CAPS.SUPPORT_STEREO_MVC: 9>: 1, <NV_ENC_CAPS.NUM_MAX_TEMPORAL_LAYERS: 10>: 4, <NV_ENC_CAPS.SUPPORT_HIERARCHICAL_PFRAMES: 11>: 1, <NV_ENC_CAPS.SUPPORT_HIERARCHICAL_BFRAMES: 12>: 1, <NV_ENC_CAPS.LEVEL_MAX: 13>: 62, <NV_ENC_CAPS.LEVEL_MIN: 14>: 10, <NV_ENC_CAPS.SEPARATE_COLOUR_PLANE: 15>: 1, <NV_ENC_CAPS.WIDTH_MAX: 16>: 4096, <NV_ENC_CAPS.HEIGHT_MAX: 17>: 4096, <NV_ENC_CAPS.SUPPORT_TEMPORAL_SVC: 18>: 1, <NV_ENC_CAPS.SUPPORT_DYN_RES_CHANGE: 19>: 1, <NV_ENC_CAPS.SUPPORT_DYN_BITRATE_CHANGE: 20>: 1, <NV_ENC_CAPS.SUPPORT_DYN_FORCE_CONSTQP: 21>: 1, <NV_ENC_CAPS.SUPPORT_DYN_RCMODE_CHANGE: 22>: 0, <NV_ENC_CAPS.SUPPORT_SUBFRAME_READBACK: 23>: 1, <NV_ENC_CAPS.SUPPORT_CONSTRAINED_ENCODING: 24>: 1, <NV_ENC_CAPS.SUPPORT_INTRA_REFRESH: 25>: 1, <NV_ENC_CAPS.SUPPORT_CUSTOM_VBV_BUF_SIZE: 26>: 1, <NV_ENC_CAPS.SUPPORT_DYNAMIC_SLICE_MODE: 27>: 1, <NV_ENC_CAPS.SUPPORT_REF_PIC_INVALIDATION: 28>: 1, <NV_ENC_CAPS.PREPROC_SUPPORT: 29>: 0, <NV_ENC_CAPS.ASYNC_ENCODE_SUPPORT: 30>: 0, <NV_ENC_CAPS.MB_NUM_MAX: 31>: 65536, <NV_ENC_CAPS.MB_PER_SEC_MAX: 32>: 983040, <NV_ENC_CAPS.SUPPORT_YUV444_ENCODE: 33>: 1, <NV_ENC_CAPS.SUPPORT_LOSSLESS_ENCODE: 34>: 1, <NV_ENC_CAPS.SUPPORT_SAO: 35>: 0, <NV_ENC_CAPS.SUPPORT_MEONLY_MODE: 36>: 1, <NV_ENC_CAPS.SUPPORT_LOOKAHEAD: 37>: 1, <NV_ENC_CAPS.SUPPORT_TEMPORAL_AQ: 38>: 1, <NV_ENC_CAPS.SUPPORT_10BIT_ENCODE: 39>: 0, <NV_ENC_CAPS.NUM_MAX_LTR_FRAMES: 40>: 8, <NV_ENC_CAPS.SUPPORT_WEIGHTED_PREDICTION: 41>: 1, <NV_ENC_CAPS.DYNAMIC_QUERY_ENCODER_CAPACITY: 42>: 100, <NV_ENC_CAPS.SUPPORT_BFRAME_REF_MODE: 43>: 3, <NV_ENC_CAPS.SUPPORT_EMPHASIS_LEVEL_MAP: 44>: 1, <NV_ENC_CAPS.WIDTH_MIN: 45>: 145, <NV_ENC_CAPS.HEIGHT_MIN: 46>: 49, <NV_ENC_CAPS.SUPPORT_MULTIPLE_REF_FRAMES: 47>: 1, <NV_ENC_CAPS.SUPPORT_ALPHA_LAYER_ENCODING: 48>: 0, <NV_ENC_CAPS.???: 49>: 1, <NV_ENC_CAPS.???: 50>: 1}\n",
      "119 batches processed\n",
      "Generating '/tmp/nsys-report-4332.qdstrm'\n",
      "[1/1] [========================100%] optimized_cvcuda_nvcodec_pybt.nsys-rep\n",
      "Generated:\n",
      "\t/home/sanjay42/sanjay/cuda/AcceleratedPythonProgramming/reports/optimized_cvcuda_nvcodec_pybt.nsys-rep\n"
     ]
    }
   ],
   "source": [
    "!nsys profile \\\n",
    "--trace cuda,nvtx,osrt,nvvideo \\\n",
    "--output reports/optimized_cvcuda_nvcodec_pybt \\\n",
    "--force-overwrite=true \\\n",
    "--python-sampling=true --python-sampling-frequency=400 \\\n",
    "python video_segmentation/main_nvtx-cvcuda-nvcodec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054df05c-393d-491f-94d0-c6c30e1ead18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's open the report file in the Nsight Systems GUI.\n",
    "\n",
    "Zooming into a single batch of the pipeline confirms that the application is invoking the Video Encode and Video Decode APIs and there are no more memory transfers from HtoD before the execution of CUDA kernels for the batch and DtoH afterwards. Filtering the memory operations to just the _pipeline_ NVTX range as before shows no Memcpy HtoD or Memcpy DtoH operations. The pipeline stage is now down to ~5.7s which is a speedup of 2.1x compared to the previous optimization step and 11.5x to the baseline code.\n",
    "\n",
    "<center><img src=images/step2/nsys_timeline_nvcodec.png></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c41a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Summary</b>\n",
    "    <p>\n",
    "        We went through another iteration of the optimization workflow by avoiding the data movement between the CPU and GPU.\n",
    "    </p>\n",
    "    <p>\n",
    "        The <i>Events View</i> feature is handy when searching for events of interest in a timeline row.\n",
    "    </p>\n",
    "    <p>\n",
    "        The Jupyterlab extension for NVIDIA Nsight tools enables you to directly profile Python code in a code cell.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3b5800",
   "metadata": {},
   "source": [
    "Please click [here](step3.ipynb) to move to the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
